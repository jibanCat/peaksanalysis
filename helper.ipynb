{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "from mifipy import mfun\n",
    "import seaborn as sns\n",
    "import lmfit\n",
    "from lmfit import Model\n",
    "from lmfit.models import GaussianModel, ParabolicModel, ExpressionModel, ExponentialGaussianModel, MoffatModel, SkewedGaussianModel\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_index_of(arrval, value):\n",
    "    return np.argmin( abs(arrval - value) )\n",
    "\n",
    "def gaussian_p(x, p):\n",
    "    amp, mean, sigma = p\n",
    "    return - amp / sigma**3 / np.sqrt(2 * np.pi) * (x - mean) * \\\n",
    "        np.exp(- (x - mean)**2. / 2 / sigma**2.)\n",
    "\n",
    "def gaussian_pp(x, p):\n",
    "    amp, mean, sigma = p\n",
    "    return amp / sigma**5 / np.sqrt(2 * np.pi) * (x - mean) ** 2. * np.exp(- (x - mean)**2. / 2 / sigma**2.) \\\n",
    "        - amp / sigma**3 / np.sqrt(2 * np.pi) * np.exp(- (x - mean)**2. / 2 / sigma**2.) \n",
    "\n",
    "# An Ugly Function to Find 5 peaks beased on 1st derivative\n",
    "def find_five_gaussians_roots(parameters):\n",
    "    try:\n",
    "        p1, p2, p3, p4, p5 = parameters.reshape(5, 3)\n",
    "    except:\n",
    "        p1, p2, p3, p4, p5 = np.array(parameters).reshape(5, 3)\n",
    "\n",
    "    def five_gaussian_p(x):\n",
    "        return sum([gaussian_p(x, p) for p in [p1, p2, p3, p4, p5]])\n",
    "    \n",
    "    def five_gaussian_pp(x):\n",
    "        return sum([gaussian_pp(x, p) for p in [p1, p2, p3, p4, p5]])\n",
    "    \n",
    "    center_list = [p1[1], p2[1], p3[1], p4[1], p5[1]]\n",
    "    opt_list = np.array(sorted([optimize.root(five_gaussian_p, i, jac=five_gaussian_pp).x[0] \n",
    "                                for i in center_list]))\n",
    "    return [opt_list[nearest_index_of(opt_list, p[1])] for p in [p1, p2, p3, p4, p5]]\n",
    "\n",
    "# update the values of peaks to replace gaussian centers\n",
    "def updating_dataframe_peaks_with_roots(df):\n",
    "    df_update = df.copy()\n",
    "    peaks_string_list = ['peak1', 'peak2', 'peak3', 'peak4', 'peak5']\n",
    "    \n",
    "    # Best fit with 5 gaussians\n",
    "    centers = pd.DataFrame([df_update.loc[:, pkstr] for pkstr in peaks_string_list]).T\n",
    "    centers.columns = ['center1', 'center2', 'center3', 'center4', 'center5']\n",
    "\n",
    "    for i in df_update.index:\n",
    "        parameters = [df_update.loc[i, par + str(j)] for j in range(1,6) for par in ['amp', 'peak', 'sigma']]\n",
    "        five_roots = find_five_gaussians_roots(parameters)\n",
    "        for root, pkstr in zip(five_roots, peaks_string_list): \n",
    "            df_update.loc[i, pkstr] = root\n",
    "\n",
    "    # concat df_update and centers\n",
    "    return pd.concat([df_update, centers], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updating_dictionary_peaks_with_roots(df):\n",
    "    df_update = df.copy()\n",
    "    peaks_string_list = ['g1_peak', 'g2_peak', 'g3_peak', 'g4_peak', 'g5_peak']\n",
    "    parameters_list = ['g' + str(j) + '_' + par for j in range(1,6) \n",
    "                                                for par in ['amplitude', 'center', 'sigma']]\n",
    "    peaks = {p : [] for p in peaks_string_list}\n",
    "    for i in df_update.index:\n",
    "        parameters = [df_update.loc[i, par] for par in parameters_list]\n",
    "        five_roots = find_five_gaussians_roots(parameters)\n",
    "\n",
    "        for root, pkstr in zip(five_roots, peaks_string_list): \n",
    "            peaks[pkstr].append(root)\n",
    "\n",
    "    # concat df_update and centers\n",
    "    return pd.concat([pd.DataFrame(peaks), df_update], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gaussian_fit_helper updater\n",
    "def update_gaussuan_helper_with_roots(result):\n",
    "    parameters = [result.best_values[prefix + par] \n",
    "        for prefix in ['g1_', 'g2_', 'g3_', 'g4_', 'g5_']\n",
    "        for par in ['amplitude', 'center', 'sigma']]\n",
    "    peak_list = [prefix + 'peak' for prefix in ['g1_', 'g2_', 'g3_', 'g4_', 'g5_']]\n",
    "    peaks = find_five_gaussians_roots(parameters)\n",
    "    for name, peak in zip(peak_list, peaks):\n",
    "        result.best_values[name] = peak\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_of(arrval, value):\n",
    "    if value < min(arrval): return 0\n",
    "    return max(np.where(arrval<=value)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussians_fit_helper(ell, cl, return_fig=False, \n",
    "                         bound_peak=False, range_guess=False, method='leastsq'):\n",
    "    '''fit power spectrum with gaussians\n",
    "    parameter: \n",
    "    return_fig=True -> return figures for fitting result\n",
    "    '''\n",
    "\n",
    "    # find better data range \n",
    "    def index_of(arrval, value):\n",
    "        if value < min(arrval): return 0\n",
    "        return max(np.where(arrval<=value)[0])\n",
    "    ix1 = index_of(ell,415.35107897)\n",
    "    ix2 = index_of(ell,670.1145778)\n",
    "    ix3 = index_of(ell,1020.37320212)\n",
    "    ix4 = index_of(ell,1311.72424321)\n",
    "    ix5 = index_of(ell,1604)\n",
    "    \n",
    "    # fits with gaussian functions \n",
    "    gauss1 = GaussianModel(prefix='g1_')\n",
    "    gauss2 = GaussianModel(prefix='g2_')\n",
    "    gauss3 = GaussianModel(prefix='g3_')\n",
    "    gauss4 = GaussianModel(prefix='g4_')\n",
    "    gauss5 = GaussianModel(prefix='g5_')\n",
    "    \n",
    "    # initial vaule \n",
    "    if range_guess==True:\n",
    "        pars = gauss1.guess(cl[:ix1], x=ell[:ix1])\n",
    "        pars += gauss2.guess(cl[ix1:ix2], x=ell[ix1:ix2])\n",
    "        pars += gauss3.guess(cl[ix2:ix3], x=ell[ix2:ix3])\n",
    "        pars += gauss4.guess(cl[ix3:ix4], x=ell[ix3:ix4])\n",
    "        pars += gauss5.guess(cl[ix4:ix5], x=ell[ix4:ix5])\n",
    "    if range_guess==False:\n",
    "        pars = gauss1.guess(cl[:ix1], x=ell[:ix1])\n",
    "        pars.update(gauss1.make_params())\n",
    "        pars['g1_amplitude'].set(1383237.6375727942)\n",
    "        pars['g1_center'].set(218.25413354426811) # 218.25413354426811\n",
    "        pars['g1_sigma'].set(97.08110083797027)\n",
    "        pars.update(gauss2.make_params())\n",
    "        pars['g2_amplitude'].set(541007.6549703055)\n",
    "        pars['g2_center'].set(532.7906394119341) # 532.7906394119341\n",
    "        pars['g2_sigma'].set(86.81717396133978)\n",
    "        pars.update(gauss3.make_params())\n",
    "        pars['g3_amplitude'].set(692701.0866300496)\n",
    "        pars['g3_center'].set(812.1818016015095) # 812.1818016015095\n",
    "        pars['g3_sigma'].set(110.16506040901645)\n",
    "        pars.update(gauss4.make_params())\n",
    "        pars['g4_amplitude'].set(243336.67891973304)\n",
    "        pars['g4_center'].set(1125.9129396789704) # 1125.9129396789704\n",
    "        pars['g4_sigma'].set(92.67204801014215)\n",
    "        pars.update(gauss5.make_params())\n",
    "        pars['g5_amplitude'].set(382571.02244731307)\n",
    "        pars['g5_center'].set(1424.7207136484815) # 1424.7207136484815\n",
    "        pars['g5_sigma'].set(183.43056355023555)\n",
    "    \n",
    "    gmod =  gauss1 + gauss2 + gauss3 + gauss4 + gauss5\n",
    "    \n",
    "    # bound the parameter\n",
    "    #pars['g1_sigma'].set(min=48.046687036192097, max=147.56693526257902)\n",
    "    #pars['g2_sigma'].set(min=6.5892775323649744, max=183.75326766522033)\n",
    "    #pars['g3_sigma'].set(min=35.193792564236674, max=183.98817016897786)\n",
    "\n",
    "    if bound_peak==True:\n",
    "        #pars['g1_center'].set(min=0, max=1700)\n",
    "        #pars['g2_center'].set(min=0, max=1700)\n",
    "        #pars['g3_center'].set(min=0, max=1700)\n",
    "        #pars['g4_center'].set(min=0, max=1700)\n",
    "        #pars['g5_center'].set(min=0, max=1700)\n",
    "        pars['g1_sigma'].set(max=1000)\n",
    "        pars['g2_sigma'].set(max=1000)\n",
    "        pars['g3_sigma'].set(max=1000)\n",
    "        pars['g4_sigma'].set(max=1000)\n",
    "        pars['g5_sigma'].set(max=1000)\n",
    "        pars['g1_amplitude'].set(min=0)\n",
    "        pars['g2_amplitude'].set(min=0)\n",
    "        pars['g3_amplitude'].set(min=0)\n",
    "        pars['g4_amplitude'].set(min=0)\n",
    "        pars['g5_amplitude'].set(min=0)\n",
    "        \n",
    "    # fit model to data array ecl \n",
    "    result = gmod.fit(cl, pars, x=ell, method=method) ## ell=ell[0:1600], ecl=ecl[0:1600]\n",
    "    if return_fig==True:\n",
    "        plt.plot(ell, cl, 'k', linestyle='None', marker='.')\n",
    "        plt.plot(ell, result.best_fit, lw=2)\n",
    "\n",
    "        # Components plots\n",
    "        comps = result.eval_components(x=ell)\n",
    "        plt.plot(ell, comps['g1_'])\n",
    "        plt.plot(ell, comps['g2_'])\n",
    "        plt.plot(ell, comps['g3_'])\n",
    "        plt.plot(ell, comps['g4_'])\n",
    "        plt.plot(ell, comps['g5_'])\n",
    "\n",
    "        plt.xlabel(r'$\\ell$')\n",
    "        plt.ylabel(r'$\\ell(\\ell+1) C_\\ell/2\\pi$')\n",
    "        plt.xlim([0., ell[-1]])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def peak_df(ell, table_ecls, lmax, lmin, method='leastsq'):\n",
    "    peak1 = []; peak2 = []; peak3 = []; peak4 = []; peak5 =[];\n",
    "    sigma1 = []; sigma2 = []; sigma3 = []; sigma4 = []; sigma5 = [];\n",
    "    amp1 = []; amp2 = []; amp3 = []; amp4 = []; amp5 = [];\n",
    "    redchi  = [];\n",
    "    for i in range(len(table_ecls)):\n",
    "        result = gaussians_fit_helper(ell[index_of(ell, lmin):index_of(ell, lmax)], \n",
    "                                      table_ecls[i][index_of(ell, lmin):index_of(ell, lmax)],\n",
    "                                      method=method)\n",
    "        parms = result.params\n",
    "        peak1.append(parms['g1_center'].value)\n",
    "        peak2.append(parms['g2_center'].value)\n",
    "        peak3.append(parms['g3_center'].value)\n",
    "        peak4.append(parms['g4_center'].value)\n",
    "        peak5.append(parms['g5_center'].value)\n",
    "        sigma1.append(parms['g1_sigma'].value)\n",
    "        sigma2.append(parms['g2_sigma'].value)\n",
    "        sigma3.append(parms['g3_sigma'].value)\n",
    "        sigma4.append(parms['g4_sigma'].value)\n",
    "        sigma5.append(parms['g5_sigma'].value)\n",
    "        amp1.append(parms['g1_amplitude'].value)\n",
    "        amp2.append(parms['g2_amplitude'].value)\n",
    "        amp3.append(parms['g3_amplitude'].value)\n",
    "        amp4.append(parms['g4_amplitude'].value)\n",
    "        amp5.append(parms['g5_amplitude'].value)\n",
    "        \n",
    "        redchi.append(result.redchi)\n",
    "    d = {'peak1': peak1,\n",
    "        'peak2' : peak2,\n",
    "        'peak3' : peak3,\n",
    "        'peak4' : peak4,\n",
    "        'peak5' : peak5,\n",
    "        'sigma1': sigma1,\n",
    "        'sigma2': sigma2,\n",
    "        'sigma3': sigma3,\n",
    "        'sigma4': sigma4,\n",
    "        'sigma5': sigma5,\n",
    "        'amp1'  : amp1,\n",
    "        'amp2'  : amp2,\n",
    "        'amp3'  : amp3,\n",
    "        'amp4'  : amp4,\n",
    "        'amp5'  : amp5,\n",
    "        'redchi': redchi}\n",
    "    df = pd.DataFrame(d)\n",
    "    del peak1, peak2, peak3, peak4, peak5, sigma1, sigma2, sigma3, sigma4, sigma5, redchi, parms, result\n",
    "    \n",
    "    # it is an ad-hoc function ... well I will fix the program next time\n",
    "    df = updating_dataframe_peaks_with_roots(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outer_outlier_help(df_test, df_train, amp_cut=True, sigma_cut=True, sort_peak=False, \n",
    "                      first_three=True):\n",
    "    # naive cutoff 3 std\n",
    "    print('len of dataframe before cutoff :', len(df_test))\n",
    "    \n",
    "    if amp_cut==True:\n",
    "        # amplitude cutoff\n",
    "        df_test = df_test[df_test['amp1'] < (df_train['amp1'].quantile(q=0.75) + (df_train['amp1'].quantile(q=0.75) - df_train['amp1'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['amp1'] > (df_train['amp1'].quantile(q=0.25) - (df_train['amp1'].quantile(q=0.75) - df_train['amp1'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['amp2'] < (df_train['amp2'].quantile(q=0.75) + (df_train['amp2'].quantile(q=0.75) - df_train['amp2'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['amp2'] > (df_train['amp2'].quantile(q=0.25) - (df_train['amp2'].quantile(q=0.75) - df_train['amp2'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['amp3'] < (df_train['amp3'].quantile(q=0.75) + (df_train['amp3'].quantile(q=0.75) - df_train['amp3'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['amp3'] > (df_train['amp3'].quantile(q=0.25) - (df_train['amp3'].quantile(q=0.75) - df_train['amp3'].quantile(q=0.25)) * 3)]\n",
    "        if first_three==False:\n",
    "            df_test = df_test[df_test['amp4'] < (df_train['amp4'].quantile(q=0.75) + (df_train['amp4'].quantile(q=0.75) - df_train['amp4'].quantile(q=0.25)) * 3)]\n",
    "            df_test = df_test[df_test['amp4'] > (df_train['amp4'].quantile(q=0.25) - (df_train['amp4'].quantile(q=0.75) - df_train['amp4'].quantile(q=0.25)) * 3)]\n",
    "            df_test = df_test[df_test['amp5'] < (df_train['amp5'].quantile(q=0.75) + (df_train['amp5'].quantile(q=0.75) - df_train['amp5'].quantile(q=0.25)) * 3)]\n",
    "            df_test = df_test[df_test['amp5'] > (df_train['amp5'].quantile(q=0.25) - (df_train['amp5'].quantile(q=0.75) - df_train['amp5'].quantile(q=0.25)) * 3)]\n",
    "        print('len of dataframe after amplitude cutoff :', len(df_test))\n",
    "    \n",
    "    if sigma_cut==True:\n",
    "        # sigma cutoff\n",
    "        df_test = df_test[df_test['sigma1'] < (df_train['sigma1'].quantile(q=0.75) + (df_train['sigma1'].quantile(q=0.75) - df_train['sigma1'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['sigma1'] > (df_train['sigma1'].quantile(q=0.25) - (df_train['sigma1'].quantile(q=0.75) - df_train['sigma1'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['sigma2'] < (df_train['sigma2'].quantile(q=0.75) + (df_train['sigma2'].quantile(q=0.75) - df_train['sigma2'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['sigma2'] > (df_train['sigma2'].quantile(q=0.25) - (df_train['sigma2'].quantile(q=0.75) - df_train['sigma2'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['sigma3'] < (df_train['sigma3'].quantile(q=0.75) + (df_train['sigma3'].quantile(q=0.75) - df_train['sigma3'].quantile(q=0.25)) * 3)]\n",
    "        df_test = df_test[df_test['sigma3'] > (df_train['sigma3'].quantile(q=0.25) - (df_train['sigma3'].quantile(q=0.75) - df_train['sigma3'].quantile(q=0.25)) * 3)]\n",
    "        if first_three==False:\n",
    "            df_test = df_test[df_test['sigma4'] < (df_train['sigma4'].quantile(q=0.75) + (df_train['sigma4'].quantile(q=0.75) - df_train['sigma4'].quantile(q=0.25)) * 3)]\n",
    "            df_test = df_test[df_test['sigma4'] > (df_train['sigma4'].quantile(q=0.25) - (df_train['sigma4'].quantile(q=0.75) - df_train['sigma4'].quantile(q=0.25)) * 3)]\n",
    "            df_test = df_test[df_test['sigma5'] < (df_train['sigma5'].quantile(q=0.75) + (df_train['sigma5'].quantile(q=0.75) - df_train['sigma5'].quantile(q=0.25)) * 3)]\n",
    "            df_test = df_test[df_test['sigma5'] > (df_train['sigma5'].quantile(q=0.25) - (df_train['sigma5'].quantile(q=0.75) - df_train['sigma5'].quantile(q=0.25)) * 3)]\n",
    "        print('len of dataframe after sigma cutoff :', len(df_test))\n",
    "    \n",
    "    if sort_peak==True:\n",
    "        # sort alignment peaks\n",
    "        df_test = df_test[df_test['peak1'] < df_test['peak2']]\n",
    "        df_test = df_test[df_test['peak1'] < df_test['peak3']]\n",
    "        df_test = df_test[df_test['peak2'] < df_test['peak3']]\n",
    "        if first_three==False:\n",
    "            df_test = df_test[df_test['peak1'] < df_test['peak4']]\n",
    "            df_test = df_test[df_test['peak1'] < df_test['peak5']]\n",
    "            df_test = df_test[df_test['peak2'] < df_test['peak4']]\n",
    "            df_test = df_test[df_test['peak2'] < df_test['peak5']]\n",
    "            df_test = df_test[df_test['peak3'] < df_test['peak4']]\n",
    "            df_test = df_test[df_test['peak3'] < df_test['peak5']]\n",
    "            df_test = df_test[df_test['peak4'] < df_test['peak5']]\n",
    "        print('len of dataframe after sorting alignment peaks :', len(df_test))\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bad_df(df, first_three=False):\n",
    "    # pandas index to list\n",
    "    pre_idx = df.index.tolist()\n",
    "\n",
    "    # outer outlier eliminating\n",
    "    df_cut = outer_outlier_help(df, df, amp_cut=True, sigma_cut=True, sort_peak=True, first_three=first_three)\n",
    "    aft_idx = df_cut.index.tolist()\n",
    "\n",
    "    # fancy indexing with boolean indicing operator !=\n",
    "    boolean_list = [idx not in aft_idx for idx in pre_idx]\n",
    "    df_bad = df[boolean_list]\n",
    "    return df_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cart_healpix(cartview, nside):\n",
    "    '''read in an matrix and return a healpix pixelization map'''\n",
    "    # Generate a flat Healpix map and angular to pixels\n",
    "    healpix = np.zeros(hp.nside2npix(nside), dtype=np.double)\n",
    "    hptheta = np.linspace(0, np.pi, num=cartview.shape[0])[:, None]\n",
    "    hpphi = np.linspace(-np.pi, np.pi, num=cartview.shape[1])\n",
    "    pix = hp.ang2pix(nside, hptheta, hpphi)\n",
    "    \n",
    "    # re-pixelize\n",
    "    healpix[pix] = np.fliplr(np.flipud(cartview))\n",
    "    \n",
    "    return healpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def healpix_rotate(healpix_map, rot):\n",
    "    # pix-vec\n",
    "    ipix = np.arange(len(healpix_map))\n",
    "    nside = np.sqrt(len(healpix_map) / 12)\n",
    "    if int(nside) != nside: return print('invalid nside');\n",
    "    nside = int(nside)\n",
    "    vec = hp.pix2vec(int(nside), ipix)\n",
    "    rot_vec = (hp.rotator.Rotator(rot=rot)).I(vec)\n",
    "    irotpix = hp.vec2pix(nside, rot_vec[0], rot_vec[1], rot_vec[2])\n",
    "    return np.copy(healpix_map[irotpix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lonlat_block_shorten(rotate, blocksize=(18, 18), xsize=1080, nside=64):\n",
    "    lon, lat, psi = rotate\n",
    "    cartview = np.zeros((xsize/2, xsize))\n",
    "    \n",
    "    # center block\n",
    "    blocksize = (blocksize[0]/360*xsize, blocksize[0]/360*xsize)\n",
    "    cartview[xsize/4 - blocksize[1]/2: xsize/4 + blocksize[1]/2, \n",
    "             xsize/2 - blocksize[0]/2: xsize/2 + blocksize[0]/2] = np.ones((blocksize[0], blocksize[1]))\n",
    "    \n",
    "    # re-pixelization\n",
    "    healpix = cart_healpix(cartview, nside)\n",
    "    del cartview\n",
    "\n",
    "    # cartview back to origin: theta \n",
    "    healpix = healpix_rotate(healpix, (0,-lat))\n",
    "    \n",
    "    return cart_healpix(cartview, nside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cute function to calculate the shift and variance of peak positions compared to bestfit $\\Lambda$CDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_df_concat(df, return_abs=False):\n",
    "    # Call in Best-Fit\n",
    "    best = np.loadtxt('../Release2/COM_PowerSpect_CMB-base-plikHM-TT-lowTEB-minimum-theory_R2.02.txt')\n",
    "    L = best.T[0]\n",
    "    CL = best.T[1]\n",
    "    C1 = ['peak1', 'peak2', 'peak3']\n",
    "    C2 = ['g1_peak', 'g2_peak', 'g3_peak']\n",
    "    \n",
    "    # Best fit with 5 gaussians\n",
    "    best = gaussians_fit_helper(L[0:1604], CL[0:1604], return_fig=False)\n",
    "    best = update_gaussuan_helper_with_roots(best)\n",
    "    shift = pd.DataFrame([df.loc[:, c1] - best.best_values[c2] for c1, c2 in zip(C1, C2)]).T\n",
    "    shift.columns = ['shift1','shift2','shift3']\n",
    "\n",
    "    # concat the total shift\n",
    "    if return_abs==True:\n",
    "        shift = pd.concat([shift, np.sum(abs(shift), axis=1), np.sqrt(np.sum(shift**2, axis=1))], axis=1)\n",
    "    else:\n",
    "        shift = pd.concat([shift, np.sum(shift, axis=1), np.sqrt(np.sum(shift**2, axis=1))], axis=1)\n",
    "    \n",
    "    shift.columns = ['shift1','shift2','shift3', 'shift_sum', 'shift_var_distance']\n",
    "    \n",
    "    # concat df and shift\n",
    "    return pd.concat([df, shift], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def self_df_concat(df, return_lensed=False, step=None, stop=None):\n",
    "    '''\n",
    "    Concentrate a given dataframe with its ['peak1':'peak3] subutracted with its mean values. \n",
    "    Users may be able to selected a divided range with slices keyword argument. \n",
    "    For a dataframe with length 140, slices=70 impies subutract 0~69 with 0~69 mean values, 70~139 with 70~139 mean values.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : dataframe.\n",
    "    return_lensed : boolean.\n",
    "    slices : int\n",
    "    Returns:\n",
    "    df : dataframe\n",
    "    -------\n",
    "    '''\n",
    "    C1 = ['peak1', 'peak2', 'peak3']\n",
    "    \n",
    "    if return_lensed==True:\n",
    "        temp = [pd.DataFrame([df.loc[i:i + step - 1, c1] - df.loc[i:i + step - 1, c1].mean() \n",
    "                       for c1 in C1]).T \n",
    "                for i in range(0, stop, step)]\n",
    "        shift = pd.concat(temp, axis=0)\n",
    "        shift.columns = ['shift1','shift2','shift3']\n",
    "    if return_lensed==False:\n",
    "        shift = pd.DataFrame([df.loc[:, c1] - df.loc[:, c1].mean() for c1 in C1]).T\n",
    "        shift.columns = ['shift1','shift2','shift3']\n",
    "\n",
    "    # concat the total shift\n",
    "    shift = pd.concat([shift, np.sum(shift, axis=1), np.sum(abs(shift), axis=1), np.sqrt(np.sum(shift**2, axis=1))], axis=1)\n",
    "    shift.columns = ['shift1','shift2','shift3', 'shift_sum', 'shift_abs_sum', 'shift_var_distance']\n",
    "    \n",
    "    # concat df and shift\n",
    "    return pd.concat([df, shift], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_sky_map(df, a, b, size, theta_size, nside, patch_size=240):\n",
    "    IMP = np.zeros(hp.nside2npix(nside), dtype=np.double)\n",
    "\n",
    "    # suboptimal choice: let bad-fit become zero\n",
    "    df_bad = bad_df(df)\n",
    "    bad_index = list(df_bad.index)\n",
    "\n",
    "    # subpotimal choice: rebulid list without bad-fit\n",
    "    shift_array = np.copy(df['shift_var_distance'].values)\n",
    "    if len(bad_index) != 0:\n",
    "        shift_array[bad_index] = 0\n",
    "    importance_list = list_rebuild(shift_array, b)\n",
    "\n",
    "    for (i,theta),importance in zip(enumerate(a), importance_list):\n",
    "        print(importance)\n",
    "        IMP += Yakitori.Shift_Block_Rotater(theta * theta_size, size, nside, \n",
    "                                            b[i], importance, patch_size=patch_size)\n",
    "    return IMP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_rebuild(index, b):\n",
    "    '''\n",
    "    index: list, the list you want to rebuild\n",
    "    b: list, the base list you want to take it as template to rebuid a list\n",
    "    return:\n",
    "    rebuild_list: list, reconstructed list based on b.\n",
    "    '''\n",
    "    # generate aggregate index\n",
    "    length = [len(B) for B in b]\n",
    "    if len(index) == np.sum(length):\n",
    "        sum_length = np.insert(np.add.accumulate(length), 0, 0) # accumulate is cool\n",
    "        return [index[sum_length[i]:sum_length[i+1]] for i,l in enumerate(sum_length[:-1])]\n",
    "    else: print ('Not the same length!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ones_circle(length):\n",
    "    circle = np.zeros((length, length))\n",
    "    y, x = np.indices((circle.shape))\n",
    "    center = np.array([(x.max() - x.min()) / 2.0, (x.max() - x.min()) / 2.0])\n",
    "    r = np.sqrt((x - center[0]) ** 2 + (y - center[1]) ** 2)\n",
    "    circle[r < length / 2] = 1\n",
    "    return circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ones_number_circle(length, number):\n",
    "\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    from matplotlib.image import pil_to_array\n",
    "    \n",
    "    # decide your font\n",
    "    fnt = ImageFont.truetype('/Library/Fonts/Georgia.ttf', int(length * 0.626)) # 135\n",
    "    circle = ones_circle(length)\n",
    "    txt = Image.new('L', size=(length, length), color=1)\n",
    "    draw = ImageDraw.Draw(txt)\n",
    "    \n",
    "    w, h = draw.textsize(number, font=fnt)\n",
    "    \n",
    "    draw.text(((length - w) / 2, (length - h) / 4), text=number, font=fnt, align='center')\n",
    "    txt = pil_to_array(txt)\n",
    "    return circle * txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy: one time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussians_fit_bounds(ell, cl, bounds,\n",
    "                         return_fig=False):\n",
    "    '''fit power spectrum with gaussians\n",
    "    parameter: \n",
    "    bounds : bounds in 2darray, [[sigma1_min, sigma1_max],[sigma2_min, sigma2_max], ...]\n",
    "    return_fig=True : return figures for fitting result\n",
    "    '''\n",
    "\n",
    "    # find better data range \n",
    "    def index_of(arrval, value):\n",
    "        if value < min(arrval): return 0\n",
    "        return max(np.where(arrval<=value)[0])\n",
    "    ix1 = index_of(ell,400)\n",
    "    ix2 = index_of(ell,600)\n",
    "    ix3 = index_of(ell,1000)\n",
    "    ix4 = index_of(ell,1300)\n",
    "    ix5 = index_of(ell,1600)\n",
    "\n",
    "    # fits with gaussian functions \n",
    "    gauss1 = GaussianModel(prefix='g1_')\n",
    "    gauss2 = GaussianModel(prefix='g2_')\n",
    "    gauss3 = GaussianModel(prefix='g3_')\n",
    "    gauss4 = GaussianModel(prefix='g4_')\n",
    "    gauss5 = GaussianModel(prefix='g5_')\n",
    "    \n",
    "    gmod =  gauss1 + gauss2 + gauss3 + gauss4 + gauss5\n",
    "\n",
    "    # initial vaule \n",
    "    pars = gauss1.guess(cl[:ix1], x=ell[:ix1])\n",
    "    pars += gauss2.guess(cl[ix1:ix2], x=ell[ix1:ix2])\n",
    "    pars += gauss3.guess(cl[ix2:ix3], x=ell[ix2:ix3])\n",
    "    pars += gauss4.guess(cl[ix3:ix4], x=ell[ix3:ix4])\n",
    "    pars += gauss5.guess(cl[ix4:ix5], x=ell[ix4:ix5])\n",
    "    \n",
    "    pars['g1_sigma'].set(min=bounds[0][0], max=bounds[0][1])\n",
    "    pars['g2_sigma'].set(min=bounds[1][0], max=bounds[1][1])\n",
    "    pars['g3_sigma'].set(min=bounds[2][0], max=bounds[2][1])\n",
    "    pars['g4_sigma'].set(min=bounds[3][0], max=bounds[3][1])\n",
    "    pars['g5_sigma'].set(min=bounds[4][0], max=bounds[4][1])\n",
    "    pars['g1_amplitude'].set(min=0)\n",
    "    pars['g2_amplitude'].set(min=0)\n",
    "    pars['g3_amplitude'].set(min=0)\n",
    "    pars['g4_amplitude'].set(min=0)\n",
    "    pars['g5_amplitude'].set(min=0)\n",
    "\n",
    "    # fit model to data array ecl \n",
    "    result = gmod.fit(cl, pars, x=ell) ## ell=ell[0:1600], ecl=ecl[0:1600]\n",
    "    if return_fig==True:\n",
    "        plt.plot(ell, cl, 'k', linestyle='None', marker='.')\n",
    "        plt.plot(ell, result.best_fit, lw=2)\n",
    "\n",
    "        # Components plots\n",
    "        comps = result.eval_components(x=ell)\n",
    "        plt.plot(ell, comps['g1_'])\n",
    "        plt.plot(ell, comps['g2_'])\n",
    "        plt.plot(ell, comps['g3_'])\n",
    "        plt.plot(ell, comps['g4_'])\n",
    "        plt.plot(ell, comps['g5_'])\n",
    "\n",
    "        plt.xlabel(r'$\\ell$')\n",
    "        plt.ylabel(r'$\\ell(\\ell+1) C_\\ell/2\\pi$')\n",
    "        plt.xlim([0., ell[-1]])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def peak_df_bounds(ell, table_ecls, bounds):\n",
    "    peak1 = []; peak2 = []; peak3 = []; peak4 = []; peak5 =[];\n",
    "    sigma1 = []; sigma2 = []; sigma3 = []; sigma4 = []; sigma5 = [];\n",
    "    amp1 = []; amp2 = []; amp3 = []; amp4 = []; amp5 = [];\n",
    "    redchi  = [];\n",
    "    for i in range(len(table_ecls)):\n",
    "        result = gaussians_fit_bounds(ell[:index_of(ell, 1700)], \n",
    "                                     cls[i][:index_of(ell, 1700)],\n",
    "                                     bounds)\n",
    "        parms = result.params\n",
    "        peak1.append(parms['g1_center'].value)\n",
    "        peak2.append(parms['g2_center'].value)\n",
    "        peak3.append(parms['g3_center'].value)\n",
    "        peak4.append(parms['g4_center'].value)\n",
    "        peak5.append(parms['g5_center'].value)\n",
    "        sigma1.append(parms['g1_sigma'].value)\n",
    "        sigma2.append(parms['g2_sigma'].value)\n",
    "        sigma3.append(parms['g3_sigma'].value)\n",
    "        sigma4.append(parms['g4_sigma'].value)\n",
    "        sigma5.append(parms['g5_sigma'].value)\n",
    "        amp1.append(parms['g1_amplitude'].value)\n",
    "        amp2.append(parms['g2_amplitude'].value)\n",
    "        amp3.append(parms['g3_amplitude'].value)\n",
    "        amp4.append(parms['g4_amplitude'].value)\n",
    "        amp5.append(parms['g5_amplitude'].value)\n",
    "        \n",
    "        redchi.append(result.redchi)\n",
    "    d = {'peak1': peak1,\n",
    "        'peak2' : peak2,\n",
    "        'peak3' : peak3,\n",
    "        'peak4' : peak4,\n",
    "        'peak5' : peak5,\n",
    "        'sigma1': sigma1,\n",
    "        'sigma2': sigma2,\n",
    "        'sigma3': sigma3,\n",
    "        'sigma4': sigma4,\n",
    "        'sigma5': sigma5,\n",
    "        'amp1'  : amp1,\n",
    "        'amp2'  : amp2,\n",
    "        'amp3'  : amp3,\n",
    "        'amp4'  : amp4,\n",
    "        'amp5'  : amp5,\n",
    "        'redchi': redchi}\n",
    "    df = pd.DataFrame(d)\n",
    "    del peak1, peak2, peak3, peak4, peak5, sigma1, sigma2, sigma3, sigma4, sigma5, redchi, parms, result\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLplot_help(y_test, y_predict):\n",
    "    '''\n",
    "    plot all columns in y_test compare with predict results (e.g. DecisionTree.predict, Knn.predict)\n",
    "    Parameter:\n",
    "    y_test: test set from test_train_split\n",
    "    y_predict: prediction result from x_test, trained by (x_train, y_train)\n",
    "    '''\n",
    "    for i, column in enumerate(y_test.columns):\n",
    "        b = np.c_[y_test[column].values, y_predict[:,i]]\n",
    "\n",
    "        plt.figure(1)\n",
    "        g = sns.JointGrid(b[:,0],b[:,1])\n",
    "        g.plot_marginals(sns.distplot, color=\".5\")\n",
    "        g.plot_joint(plt.hexbin, bins='log', gridsize=30, cmap=cold_cmap, extent=[0, np.max(b[:,0]), 0, np.max(b[:,0])])\n",
    "        a = np.linspace(min(b[:,0]),max(b[:,0]),20)\n",
    "        plt.plot(a,a,'k--')\n",
    "\n",
    "        plt.title(column)\n",
    "        plt.xlabel('$\\sigma _{test}$', fontsize=16)\n",
    "        plt.ylabel('$\\sigma _{predict}$', fontsize=16)\n",
    "        plt.xlim([min(b[:,0]),np.max(b[:,0])])\n",
    "        plt.ylim([min(b[:,0]),np.max(b[:,0])])\n",
    "\n",
    "        cax = g.fig.add_axes([1, 0.20, .01, 0.5])\n",
    "        cb = plt.colorbar(cax=cax)\n",
    "        cb.set_label('$\\log_{10}(\\mathcal{N})$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussians_fit_train(ell, cl, center, sig_pred, return_fig=False):\n",
    "    '''fit power spectrum with gaussians\n",
    "    parameter: \n",
    "    center: numpy array 5 length\n",
    "    sig_pred: numpy array 5 length\n",
    "    return_fig=True -> return figures for fitting result\n",
    "    '''\n",
    "    a = 1700/2/np.sum(sig_pred)\n",
    "    ix1a, ix1b = index_of(ell, center[0] - a*sig_pred[0]), index_of(ell, center[0] + a*sig_pred[0])\n",
    "    ix2a, ix2b = index_of(ell, center[1] - a*sig_pred[1]), index_of(ell, center[1] + a*sig_pred[1])\n",
    "    ix3a, ix3b = index_of(ell, center[2] - a*sig_pred[2]), index_of(ell, center[2] + a*sig_pred[2])\n",
    "    ix4a, ix4b = index_of(ell, center[3] - a*sig_pred[3]), index_of(ell, center[3] + a*sig_pred[3])\n",
    "    ix5a, ix5b = index_of(ell, center[4] - a*sig_pred[4]), index_of(ell, center[4] + a*sig_pred[4])\n",
    "\n",
    "    \n",
    "    # fits with gaussian functions \n",
    "    gauss1 = GaussianModel(prefix='g1_')\n",
    "    gauss2 = GaussianModel(prefix='g2_')\n",
    "    gauss3 = GaussianModel(prefix='g3_')\n",
    "    gauss4 = GaussianModel(prefix='g4_')\n",
    "    gauss5 = GaussianModel(prefix='g5_')\n",
    "    gmod =  gauss1 + gauss2 + gauss3 + gauss4 + gauss5\n",
    "\n",
    "    # initial vaule \n",
    "    try:\n",
    "        pars = gauss1.guess(cl[ix1a:ix1b], x=ell[ix1a:ix1b])\n",
    "        pars += gauss2.guess(cl[ix2a:ix2b], x=ell[ix2a:ix2b])\n",
    "        pars += gauss3.guess(cl[ix3a:ix3b], x=ell[ix3a:ix3b])\n",
    "        pars += gauss4.guess(cl[ix4a:ix4b], x=ell[ix4a:ix4b])\n",
    "        pars += gauss5.guess(cl[ix5a:ix5b], x=ell[ix5a:ix5b])\n",
    "    except:\n",
    "        print([ix1a, ix1b, ix2a, ix2b, ix3a, ix3b, ix4a, ix4b, ix5a, ix5b])\n",
    "        pars = gauss1.guess(cl[ix1a:ix1b], x=ell[ix1a:ix1b])\n",
    "        pars += gauss2.guess(cl[ix1b:ix2b], x=ell[ix1b:ix2b])\n",
    "        pars += gauss3.guess(cl[ix2b:ix3b], x=ell[ix2b:ix3b])\n",
    "        pars += gauss4.guess(cl[ix3b:ix4b], x=ell[ix3b:ix4b])\n",
    "        pars += gauss5.guess(cl[ix4b:ix5b], x=ell[ix4b:ix5b])\n",
    "        print('->', [ix1a, ix1b, ix1b, ix2b, ix2b, ix3b, ix3b, ix4b, ix4b, ix5b])\n",
    "        \n",
    "    # bound the parameter\n",
    "    pars['g1_center'].set(min=0)\n",
    "    pars['g2_center'].set(min=0)\n",
    "    pars['g3_center'].set(min=0)\n",
    "    pars['g4_center'].set(min=0)\n",
    "    pars['g5_center'].set(min=0)\n",
    "    pars['g1_sigma'].set(min=0)\n",
    "    pars['g2_sigma'].set(min=0)\n",
    "    pars['g3_sigma'].set(min=0)\n",
    "    pars['g4_sigma'].set(min=0)\n",
    "    pars['g5_sigma'].set(min=0)\n",
    "    pars['g1_amplitude'].set(min=0)\n",
    "    pars['g2_amplitude'].set(min=0)\n",
    "    pars['g3_amplitude'].set(min=0)\n",
    "    pars['g4_amplitude'].set(min=0)\n",
    "    pars['g5_amplitude'].set(min=0)\n",
    "        \n",
    "    # fit model to data array ecl \n",
    "    result = gmod.fit(cl, pars, x=ell) ## ell=ell[0:1600], ecl=ecl[0:1600]\n",
    "    if return_fig==True:\n",
    "        plt.plot(ell, cl, 'k', linestyle='None', marker='.')\n",
    "        plt.plot(ell, result.best_fit, lw=2)\n",
    "\n",
    "        # Components plots\n",
    "        comps = result.eval_components(x=ell)\n",
    "        plt.plot(ell, comps['g1_'])\n",
    "        plt.plot(ell, comps['g2_'])\n",
    "        plt.plot(ell, comps['g3_'])\n",
    "        plt.plot(ell, comps['g4_'])\n",
    "        plt.plot(ell, comps['g5_'])\n",
    "\n",
    "        plt.xlabel(r'$\\ell$')\n",
    "        plt.ylabel(r'$\\ell(\\ell+1) C_\\ell/2\\pi$')\n",
    "        plt.xlim([0., ell[-1]])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def peak_df_train(ell, table_ecls, data, Regressor, bound_peak=False):\n",
    "    '''\n",
    "    input ell and cls with dataframe of previous run, plus ML regressor fit, \n",
    "    expect having good initial value prediction.\n",
    "    you should run simulations to get the G(center-> sigma) mapping relation,\n",
    "    and use \n",
    "    '''\n",
    "    peak1 = []; peak2 = []; peak3 = []; peak4 = []; peak5 =[];\n",
    "    sigma1 = []; sigma2 = []; sigma3 = []; sigma4 = []; sigma5 = [];\n",
    "    amp1 = []; amp2 = []; amp3 = []; amp4 = []; amp5 = [];\n",
    "    redchi  = [];\n",
    "    \n",
    "    # ML prediction based on center positions\n",
    "    center = data.loc[:,'peak1':'peak5'].values\n",
    "    center = np.array([sorted(T) for T in center]) ## sorting gaussian centers prevent bad prediction\n",
    "    sig_pred = Regressor.predict(center)  ## ML prediction\n",
    "\n",
    "    for i in range(len(table_ecls)):\n",
    "        if bound_peak==True:\n",
    "            result = gaussians_fit_helper(ell[:index_of(ell, 1700)], \n",
    "                                         cls[i][:index_of(ell, 1700)],\n",
    "                                         bound_peak=bound_peak)\n",
    "        else:\n",
    "            result = gaussians_fit_train(ell[:index_of(ell, 1700)], \n",
    "                                         cls[i][:index_of(ell, 1700)],\n",
    "                                         center[i], sig_pred[i])\n",
    "        parms = result.params\n",
    "        peak1.append(parms['g1_center'].value)\n",
    "        peak2.append(parms['g2_center'].value)\n",
    "        peak3.append(parms['g3_center'].value)\n",
    "        peak4.append(parms['g4_center'].value)\n",
    "        peak5.append(parms['g5_center'].value)\n",
    "        sigma1.append(parms['g1_sigma'].value)\n",
    "        sigma2.append(parms['g2_sigma'].value)\n",
    "        sigma3.append(parms['g3_sigma'].value)\n",
    "        sigma4.append(parms['g4_sigma'].value)\n",
    "        sigma5.append(parms['g5_sigma'].value)\n",
    "        amp1.append(parms['g1_amplitude'].value)\n",
    "        amp2.append(parms['g2_amplitude'].value)\n",
    "        amp3.append(parms['g3_amplitude'].value)\n",
    "        amp4.append(parms['g4_amplitude'].value)\n",
    "        amp5.append(parms['g5_amplitude'].value)\n",
    "        \n",
    "        redchi.append(result.redchi)\n",
    "    d = {'peak1': peak1,\n",
    "        'peak2' : peak2,\n",
    "        'peak3' : peak3,\n",
    "        'peak4' : peak4,\n",
    "        'peak5' : peak5,\n",
    "        'sigma1': sigma1,\n",
    "        'sigma2': sigma2,\n",
    "        'sigma3': sigma3,\n",
    "        'sigma4': sigma4,\n",
    "        'sigma5': sigma5,\n",
    "        'amp1'  : amp1,\n",
    "        'amp2'  : amp2,\n",
    "        'amp3'  : amp3,\n",
    "        'amp4'  : amp4,\n",
    "        'amp5'  : amp5,\n",
    "        'redchi': redchi}\n",
    "    df = pd.DataFrame(d)\n",
    "    del peak1, peak2, peak3, peak4, peak5, sigma1, sigma2, sigma3, sigma4, sigma5, redchi, parms, result\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ordinary scipy optimize fitting\n",
    "lmin, lmax, lbinwidth = 0. , 1700., 20. # not sure the value\n",
    "\n",
    "# 5 mixture gaussian models\n",
    "def model(x, norm1, mean1, sigma1, \n",
    "          norm2, mean2, sigma2, \n",
    "          norm3, mean3, sigma3, \n",
    "          norm4, mean4, sigma4, \n",
    "          norm5, mean5, sigma5):\n",
    "    \n",
    "    gaussian1 = norm1*lbinwidth/(2.*np.pi)**0.5/sigma1 * np.exp(-0.5*((x-mean1)/sigma1)**2)\n",
    "    gaussian2 = norm2*lbinwidth/(2.*np.pi)**0.5/sigma2 * np.exp(-0.5*((x-mean2)/sigma2)**2)\n",
    "    gaussian3 = norm3*lbinwidth/(2.*np.pi)**0.5/sigma3 * np.exp(-0.5*((x-mean3)/sigma3)**2)\n",
    "    gaussian4 = norm4*lbinwidth/(2.*np.pi)**0.5/sigma4 * np.exp(-0.5*((x-mean4)/sigma4)**2)\n",
    "    gaussian5 = norm5*lbinwidth/(2.*np.pi)**0.5/sigma5 * np.exp(-0.5*((x-mean5)/sigma5)**2)\n",
    "    return gaussian1 + gaussian2 + gaussian3 + gaussian4 + gaussian5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function with 2 chosen variable \n",
    "def cost_x_y(x, y, position, ell, cl, p):\n",
    "    '''keep only 2 variables for cost function. you are able to select which you want to keep by position=(px,py)'''\n",
    "    p[position[0]] = x\n",
    "    p[position[1]] = y\n",
    "    expected = model(ell,p[0],p[1],p[2],\n",
    "                     p[3],p[4],p[5],\n",
    "                     p[6],p[7],p[8],\n",
    "                     p[9],p[10],p[11],\n",
    "                     p[12],p[13],p[14])\n",
    "    \n",
    "    clerr = cl**0.5\n",
    "    delta = (cl[cl>0] - expected[cl>0])/clerr[cl>0]\n",
    "    return (delta**2).sum()\n",
    "\n",
    "def pars_contour(position, ell, cl, amp, center, sigma, p):\n",
    "    label = ['amp', 'center', 'sigma']\n",
    "    X = [amp, center, sigma][position[0]%3]\n",
    "    Y = [amp, center, sigma][position[1]%3]\n",
    "    XV, YV = np.meshgrid(X, Y)\n",
    "    img = np.array([cost_x_y(x, y, position=position, \n",
    "        ell=ell, cl=cl, p=p) for x, y in zip(XV.ravel(), YV.ravel())]).reshape(size, size)\n",
    "    \n",
    "    # img plot\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flipud(img), cmap='terrain', aspect='auto', extent=[X.min(), X.max(), Y.min(), Y.max()]); \n",
    "    plt.colorbar();\n",
    "    plt.xlabel(label[position[0]%3] + str(position[0]//3 + 1))\n",
    "    plt.ylabel(label[position[1]%3] + str(position[1]//3 + 1))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # contour plot\n",
    "    plt.figure()\n",
    "    plt.contourf(XV, YV, img, 35, alpha=.75, cmap=cold_cmap)\n",
    "    plt.colorbar()\n",
    "    ctr = plt.contour(XV, YV, img, 35, colors='gray', linewidth=.1, linestyles='dotted')\n",
    "    plt.clabel(ctr, fontsize=8)\n",
    "    plt.xlabel(label[position[0]%3] + str(position[0]//3 + 1))\n",
    "    plt.ylabel(label[position[1]%3] + str(position[1]//3 + 1))\n",
    "    plt.tight_layout()\n",
    "    del img, X, Y, XV, YV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lmfit\n",
    "from lmfit.models import PowerLawModel, LinearModel\n",
    "\n",
    "def NPB_fit(phi_size, cla_path, clb_path, start_ell=1080,\n",
    "           return_result=False, return_figure=True, return_linear=False, compare_best=False):\n",
    "    '''ClB-ClA, and fit this curve with power law'''\n",
    "    \n",
    "    # Load in cla and ell array\n",
    "    if compare_best==False: clA = np.loadtxt(cla_path)\n",
    "    clB = np.loadtxt(clb_path)\n",
    "    ell = np.arange(0,2500,360//phi_size)\n",
    "    \n",
    "    # Load in best-fit\n",
    "    if compare_best==True: \n",
    "        try: best = np.loadtxt(cla_path)\n",
    "        except: print('Please use cla as best-fit input path :)')\n",
    "        L = best.T[0]\n",
    "        clA = (best.T[1] / L / (L + 1) * 2 * np.pi)\n",
    "        \n",
    "        # scalling \n",
    "        clA = clA[0:2500:360//phi_size][0:len(clB)]\n",
    "\n",
    "    # Get the mean value\n",
    "    clA_mean = np.sum(clA, axis=0)/len(clA)\n",
    "    clB_mean = np.sum(clB, axis=0)/len(clB)\n",
    "\n",
    "    # Difference\n",
    "    Difference = clB_mean - clA_mean\n",
    "    Difference = Difference[index_of(ell, start_ell):index_of(ell, 2500)] # elimnate singularity\n",
    "    ell = ell[index_of(ell, start_ell):index_of(ell, 2500)]\n",
    "\n",
    "    ## PowerLaw\n",
    "    power1 = PowerLawModel(prefix='p1_')\n",
    "    pars = power1.guess(Difference, x=ell)\n",
    "    result = power1.fit(Difference, pars, x=ell)\n",
    "    if return_figure==True:\n",
    "        plt.figure()\n",
    "        plt.loglog(ell,Difference,'k',ls='',marker='.')\n",
    "        plt.loglog(ell, result.best_fit,lw=2)\n",
    "        plt.xlabel(r'$\\ell$')\n",
    "        plt.ylabel(r'$C_\\ell^B - C_\\ell^A \\times b_{\\ell}^2$')\n",
    "        print(result.fit_report())\n",
    "    \n",
    "    if return_linear==True:\n",
    "        ## Linear\n",
    "        ell = np.log10(ell)\n",
    "        Difference = np.log10(Difference)\n",
    "        lin1 = LinearModel(prefix='l1_')\n",
    "        pars = lin1.guess(Difference, x=ell)\n",
    "        l1_result = lin1.fit(Difference, pars, x=ell)\n",
    "\n",
    "        if return_figure==True:\n",
    "            plt.figure()\n",
    "            plt.plot(ell, Difference,'k',ls='',marker='.')\n",
    "            plt.plot(ell, l1_result.best_fit,lw=2)\n",
    "            plt.xlabel(r'$\\ell$')\n",
    "            plt.ylabel(r'$C_\\ell^B - C_\\ell^A \\times b_{\\ell}^2$')\n",
    "            print(l1_result.fit_report())\n",
    "        return l1_result\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scipy_gaussian_helper(ell, cl, method, maxiter=None):\n",
    "    # test with one of the cls\n",
    "    cl = cl[0: index_of(ell, 1604)]\n",
    "    ell = ell[0: index_of(ell, 1604)]\n",
    "    clerr = cl**0.5\n",
    "\n",
    "    # ordinary scipy optimize fitting\n",
    "    lmin, lmax, lbinwidth = 0. , 1604., ell[1] - ell[0] # not sure the value\n",
    "\n",
    "    # gausian function\n",
    "    def gaussian(x, norm, mean, sigma):\n",
    "        return norm/(2.*np.pi)**0.5/sigma * np.exp(-0.5*((x - mean)/sigma)**2)\n",
    "\n",
    "    # 5 mixture gaussian models\n",
    "    def model(x, norm1, mean1, sigma1, \n",
    "              norm2, mean2, sigma2, \n",
    "              norm3, mean3, sigma3, \n",
    "              norm4, mean4, sigma4, \n",
    "              norm5, mean5, sigma5):\n",
    "\n",
    "        gaussian1 = gaussian(x, norm1, mean1, sigma1)\n",
    "        gaussian2 = gaussian(x, norm2, mean2, sigma2)\n",
    "        gaussian3 = gaussian(x, norm3, mean3, sigma3)\n",
    "        gaussian4 = gaussian(x, norm4, mean4, sigma4)\n",
    "        gaussian5 = gaussian(x, norm5, mean5, sigma5)\n",
    "        return gaussian1 + gaussian2 + gaussian3 + gaussian4 + gaussian5\n",
    "\n",
    "    # Cost function \n",
    "    def cost(p):\n",
    "        expected = model(ell,p[0],p[1],p[2],\n",
    "                         p[3],p[4],p[5],\n",
    "                         p[6],p[7],p[8],\n",
    "                         p[9],p[10],p[11],\n",
    "                         p[12],p[13],p[14])\n",
    "\n",
    "        delta = (cl[cl>0] - expected[cl>0])/clerr[cl>0]\n",
    "        return (delta**2).sum()\n",
    "\n",
    "    # setting initial values\n",
    "    p_init = np.array([1401655.7262623503, 218.25413354426811, 98.93376062262756,\n",
    "                       523354.98339826055, 532.7906394119341, 84.4019228730886,\n",
    "                       707106.4789739547, 812.1818016015095, 112.37624448976163,\n",
    "                       252763.19470124107, 1125.9129396789704, 91.72842845856799,\n",
    "                       291150.7466517695, 1424.7207136484815, 146.01454002758945])\n",
    "    r = opt.minimize(cost, p_init, method=method, options={'maxiter': maxiter}) #L-BFGS-B, Powell, SLSQP,\n",
    "    \n",
    "    if r.success:\n",
    "        cx = np.linspace(lmin, lmax, 1604.)\n",
    "        cy = model(cx,r.x[0],r.x[1],r.x[2],\n",
    "                   r.x[3],r.x[4],r.x[5],\n",
    "                   r.x[6],r.x[7],r.x[8],\n",
    "                   r.x[9],r.x[10],r.x[11],\n",
    "                   r.x[12],r.x[13],r.x[14])\n",
    "        plt.plot(ell, cl, ls='', marker='.', color='black')\n",
    "        plt.plot(cx, cy)\n",
    "\n",
    "        # component plots\n",
    "        plt.plot(cx, gaussian(cx, r.x[0], r.x[1], r.x[2]))\n",
    "        plt.plot(cx, gaussian(cx, r.x[3], r.x[4], r.x[5]))\n",
    "        plt.plot(cx, gaussian(cx, r.x[6], r.x[7], r.x[8]))\n",
    "        plt.plot(cx, gaussian(cx, r.x[9], r.x[10], r.x[11]))    \n",
    "        plt.plot(cx, gaussian(cx, r.x[12], r.x[13], r.x[14]))\n",
    "        \n",
    "        return r\n",
    "    else:\n",
    "        print(r.message)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
